{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7q9tsZ4hqufF"
   },
   "source": [
    "# **COVID 19 Impact on Ecommerce ASSIGNMENT**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Assignment\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upjKWap6s37e"
   },
   "source": [
    "**Team:** Team Vishleshi\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7GEECcAs-kS"
   },
   "source": [
    "## **Abstract**\n",
    "\n",
    "Foo bar..\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGf9xkiWueBr"
   },
   "source": [
    "## **Problem Statement & Business Context**\n",
    "\n",
    "The details of the problem context and the analytical overview can be found HERE\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0UYAVPKv2Ae"
   },
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81QwC8H5wEqC"
   },
   "source": [
    "### Print Formats & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUajaU7HuiUU"
   },
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3901,
     "status": "ok",
     "timestamp": 1592031244782,
     "user": {
      "displayName": "Abhinav Kimothi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggk-AW7HbPpuPwGfd4mOddPDI2UV_apsStpfn4lsA=s64",
      "userId": "13208395926076107019"
     },
     "user_tz": -330
    },
    "id": "KeQUnFgCwP99",
    "outputId": "23ed15bd-16a8-45e7-ac9d-64d4eb10ddc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[91mSolving for the current objective we will be using python version 3.7.6\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import ttest_1samp, ttest_ind, mannwhitneyu, levene, shapiro\n",
    "from statsmodels.stats.power import ttest_power\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "import statsmodels.api as sm\n",
    "from   statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy.stats import zscore\n",
    "from sklearn import svm\n",
    "from platform import python_version\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gensim.corpora  as corpora\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "import glob\n",
    "\n",
    "print (color.BOLD + color.RED + \"Solving for the current objective we will be using python version \" + python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uAu8W5GxwfHh"
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vG8TlGY6wwAN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>hi since recruiter lead permission approve req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>connection with icon</td>\n",
       "      <td>icon dear please setup icon per icon engineers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>work experience user</td>\n",
       "      <td>work experience user hi work experience studen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>requesting for meeting</td>\n",
       "      <td>requesting meeting hi please help follow equip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reset passwords for external accounts</td>\n",
       "      <td>re expire days hi ask help update passwords co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  \\\n",
       "0                                    NaN   \n",
       "1                   connection with icon   \n",
       "2                   work experience user   \n",
       "3                 requesting for meeting   \n",
       "4  reset passwords for external accounts   \n",
       "\n",
       "                                                body  \n",
       "0  hi since recruiter lead permission approve req...  \n",
       "1  icon dear please setup icon per icon engineers...  \n",
       "2  work experience user hi work experience studen...  \n",
       "3  requesting meeting hi please help follow equip...  \n",
       "4  re expire days hi ask help update passwords co...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('all_tickets_processed_3topics.csv')\n",
    "data.drop(['Target'],axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bigram & Trigram Modes Using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hi',\n",
       "  'since',\n",
       "  'recruiter',\n",
       "  'lead',\n",
       "  'permission',\n",
       "  'approve',\n",
       "  'requisitions',\n",
       "  'makes',\n",
       "  'please',\n",
       "  'make',\n",
       "  'thanks',\n",
       "  'recruiter']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sent_to_words (sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence),deacc=True))\n",
    "        \n",
    "data_words=list(sent_to_words(data['body']))\n",
    "data_words[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'since', 'recruiter', 'lead', 'permission', 'approve', 'requisitions', 'makes', 'please', 'make', 'thanks', 'recruiter']\n"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "print (trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Removing StopWords\n",
    "nltk.download('stopwords')\n",
    "stop_words=list(stopwords.words('english'))\n",
    "# Removing Stop Words\n",
    "stop_words=stop_words + ['hi','regards','thank','hello','sent','kind','help','dear','best','let','please','could','thanks']\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmitization(texts):\n",
    "    accepted_tags=['NOUN','ADJ','VERB','ADV']\n",
    "    texts_out=[]\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in accepted_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['since', 'recruiter', 'lead', 'permission', 'approve', 'requisitions', 'makes', 'make', 'recruiter']]\n"
     ]
    }
   ],
   "source": [
    "data_words =  remove_stopwords(data_words)\n",
    "data_words_bigrams =  make_bigrams(data_words)\n",
    "\n",
    "print (data_words_bigrams[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1)]]\n",
      "[[('approve', 1), ('lead', 1), ('make', 1), ('makes', 1), ('permission', 1), ('recruiter', 2), ('requisitions', 1), ('since', 1)]]\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(data_words)\n",
    "texts =  data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print (corpus[:1])\n",
    "print ([[(id2word[id],freq) for id, freq in cp] for cp in corpus[:1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.023*\"access\" + 0.022*\"october\" + 0.022*\"report\" + 0.021*\"sa\" + 0.017*\"si\" + 0.016*\"tuesday\" + 0.016*\"wednesday\" + 0.016*\"thursday\" + 0.015*\"officer\" + 0.015*\"la\"')\n",
      "\n",
      "\n",
      "(1, '0.015*\"issue\" + 0.012*\"access\" + 0.012*\"error\" + 0.009*\"en\" + 0.009*\"monday\" + 0.007*\"message\" + 0.007*\"october\" + 0.007*\"problem\" + 0.007*\"need\" + 0.007*\"cannot\"')\n",
      "\n",
      "\n",
      "(2, '0.031*\"approval\" + 0.027*\"leave\" + 0.026*\"submit\" + 0.019*\"days\" + 0.017*\"manager\" + 0.017*\"action\" + 0.016*\"annual\" + 0.014*\"submitted\" + 0.014*\"approve\" + 0.014*\"week\"')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3,\n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "for topic in lda_model.print_topics():\n",
    "    print (topic)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perpexity score is -7.521787 and Coherence score is 0.530786\n"
     ]
    }
   ],
   "source": [
    "doc_lda = lda_model[corpus]\n",
    "perpexity = lda_model.log_perplexity(corpus)\n",
    "coherence_model = gensim.models.ldamodel.CoherenceModel(model=lda_model, \n",
    "                                texts=data_words,\n",
    "                                dictionary=id2word,\n",
    "                                coherence='c_v')\n",
    "\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print ('Perpexity score is %f and Coherence score is %f'%(perpexity,coherence_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimal number of topics for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = gensim.models.ldamodel.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                                        corpus=corpus, \n",
    "                                                        texts=data_words, \n",
    "                                                        start=3, \n",
    "                                                        limit=4, \n",
    "                                                        step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show graph\n",
    "limit=4; start=3; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics = 3  has Coherence Value of 0.4705\n",
      "(0, '0.013*\"sa\" + 0.012*\"si\" + 0.011*\"change\" + 0.010*\"la\" + 0.010*\"pm\" + 0.009*\"tuesday\" + 0.008*\"error\" + 0.008*\"senior\" + 0.008*\"wednesday\" + 0.008*\"engineer\"')\n",
      "\n",
      "\n",
      "(1, '0.015*\"issue\" + 0.015*\"access\" + 0.009*\"ext\" + 0.008*\"issues\" + 0.008*\"problem\" + 0.007*\"thursday\" + 0.007*\"tuesday\" + 0.007*\"wednesday\" + 0.007*\"high\" + 0.007*\"en\"')\n",
      "\n",
      "\n",
      "(2, '0.021*\"error\" + 0.015*\"manager\" + 0.011*\"approval\" + 0.011*\"monday\" + 0.010*\"report\" + 0.010*\"submit\" + 0.009*\"log\" + 0.008*\"leave\" + 0.007*\"days\" + 0.007*\"request\"')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "    \n",
    "# Select the model and print the topics\n",
    "optimal_model = model_list[0]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "\n",
    "for topic in optimal_model.print_topics(num_words=10):\n",
    "    print (topic)\n",
    "    print ('\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the dominant topic in each sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6643</td>\n",
       "      <td>sa, si, change, la, pm, tuesday, error, senior...</td>\n",
       "      <td>[since, recruiter, lead, permission, approve, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8779</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>[icon, setup, icon, per, icon, engineers, deta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>[work, experience, user, work, experience, stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8935</td>\n",
       "      <td>error, manager, approval, monday, report, subm...</td>\n",
       "      <td>[requesting, meeting, follow, equipments, cabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>error, manager, approval, monday, report, subm...</td>\n",
       "      <td>[expire, days, ask, update, passwords, colleag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>[verification, warning, got, attached, address...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9220</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>[looks, blacklisted, receiving, mails, anymore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9941</td>\n",
       "      <td>sa, si, change, la, pm, tuesday, error, senior...</td>\n",
       "      <td>[prod, tunneling, va, la, tunneling, la, host,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>error, manager, approval, monday, report, subm...</td>\n",
       "      <td>[modules, report, report, cost, much]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8206</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>[passwords, client, passwords]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             0.0              0.6643   \n",
       "1            1             1.0              0.8779   \n",
       "2            2             1.0              0.9381   \n",
       "3            3             2.0              0.8935   \n",
       "4            4             2.0              0.3682   \n",
       "5            5             1.0              0.6953   \n",
       "6            6             1.0              0.9220   \n",
       "7            7             0.0              0.9941   \n",
       "8            8             2.0              0.8783   \n",
       "9            9             1.0              0.8206   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  sa, si, change, la, pm, tuesday, error, senior...   \n",
       "1  issue, access, ext, issues, problem, thursday,...   \n",
       "2  issue, access, ext, issues, problem, thursday,...   \n",
       "3  error, manager, approval, monday, report, subm...   \n",
       "4  error, manager, approval, monday, report, subm...   \n",
       "5  issue, access, ext, issues, problem, thursday,...   \n",
       "6  issue, access, ext, issues, problem, thursday,...   \n",
       "7  sa, si, change, la, pm, tuesday, error, senior...   \n",
       "8  error, manager, approval, monday, report, subm...   \n",
       "9  issue, access, ext, issues, problem, thursday,...   \n",
       "\n",
       "                                                Text  \n",
       "0  [since, recruiter, lead, permission, approve, ...  \n",
       "1  [icon, setup, icon, per, icon, engineers, deta...  \n",
       "2  [work, experience, user, work, experience, stu...  \n",
       "3  [requesting, meeting, follow, equipments, cabl...  \n",
       "4  [expire, days, ask, update, passwords, colleag...  \n",
       "5  [verification, warning, got, attached, address...  \n",
       "6  [looks, blacklisted, receiving, mails, anymore...  \n",
       "7  [prod, tunneling, va, la, tunneling, la, host,...  \n",
       "8              [modules, report, report, cost, much]  \n",
       "9                     [passwords, client, passwords]  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        \n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(data_words)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    21842\n",
       "0.0    15050\n",
       "2.0    11657\n",
       "Name: Dominant_Topic, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dominant_topic['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most representative document for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>sa, si, change, la, pm, tuesday, error, senior...</td>\n",
       "      <td>[complete, tab, look, come, back, log, ticket,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>[friday, november, pm, starter, form, complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>error, manager, approval, monday, report, subm...</td>\n",
       "      <td>[thursday, december, backup, storage, disk, re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0              0.9989   \n",
       "1        1.0              0.9964   \n",
       "2        2.0              0.9968   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  sa, si, change, la, pm, tuesday, error, senior...   \n",
       "1  issue, access, ext, issues, problem, thursday,...   \n",
       "2  error, manager, approval, monday, report, subm...   \n",
       "\n",
       "                                                Text  \n",
       "0  [complete, tab, look, come, back, log, ticket,...  \n",
       "1  [friday, november, pm, starter, form, complete...  \n",
       "2  [thursday, december, backup, storage, disk, re...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "#The tabular output above actually has 20 rows, one each for a topic. It has the topic number, \n",
    "#the keywords, and the most representative document. The Perc_Contribution column is nothing \n",
    "#but the percentage contribution of the topic in the given document.\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic distribution across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>sa, si, change, la, pm, tuesday, error, senior...</td>\n",
       "      <td>15050.0</td>\n",
       "      <td>0.3100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>21842.0</td>\n",
       "      <td>0.4499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>11657.0</td>\n",
       "      <td>0.2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>error, manager, approval, monday, report, subm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>error, manager, approval, monday, report, subm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48544.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48545.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>error, manager, approval, monday, report, subm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48546.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48547.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>error, manager, approval, monday, report, subm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48548.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>issue, access, ext, issues, problem, thursday,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48549 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Dominant_Topic                                     Topic_Keywords  \\\n",
       "0.0                 0.0  sa, si, change, la, pm, tuesday, error, senior...   \n",
       "1.0                 1.0  issue, access, ext, issues, problem, thursday,...   \n",
       "2.0                 1.0  issue, access, ext, issues, problem, thursday,...   \n",
       "3.0                 2.0  error, manager, approval, monday, report, subm...   \n",
       "4.0                 2.0  error, manager, approval, monday, report, subm...   \n",
       "...                 ...                                                ...   \n",
       "48544.0             1.0  issue, access, ext, issues, problem, thursday,...   \n",
       "48545.0             2.0  error, manager, approval, monday, report, subm...   \n",
       "48546.0             1.0  issue, access, ext, issues, problem, thursday,...   \n",
       "48547.0             2.0  error, manager, approval, monday, report, subm...   \n",
       "48548.0             1.0  issue, access, ext, issues, problem, thursday,...   \n",
       "\n",
       "         Num_Documents  Perc_Documents  \n",
       "0.0            15050.0          0.3100  \n",
       "1.0            21842.0          0.4499  \n",
       "2.0            11657.0          0.2401  \n",
       "3.0                NaN             NaN  \n",
       "4.0                NaN             NaN  \n",
       "...                ...             ...  \n",
       "48544.0            NaN             NaN  \n",
       "48545.0            NaN             NaN  \n",
       "48546.0            NaN             NaN  \n",
       "48547.0            NaN             NaN  \n",
       "48548.0            NaN             NaN  \n",
       "\n",
       "[48549 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topics.to_csv('alltickets_three_topics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud to Identify Prominent Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WordListCorpusReader' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-d6f9cae39f91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[0mbackground_color\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                 min_font_size = 10).generate(comment_words) \n\u001b[0m",
      "\u001b[1;32mC:\\MachineLearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \"\"\"\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MachineLearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \"\"\"\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MachineLearning\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_word_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m             \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munigrams_and_bigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_plurals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocation_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'WordListCorpusReader' object is not iterable"
     ]
    }
   ],
   "source": [
    "comment_words = 'order leaver access update officer site work client report administrator card location confluence action link floor possible rights starter purchase upgrade accounts reports mobile message tester application process cards approver annual tried owner monitor updates testing folder needs approval registered setup discipline files password software currently review consultant updated following two connect content additional check device logged server tickets changes call get colleagues section purchased th communication members period screen'\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(comment_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM0Zc2kBcQUrS/1SwHJ7iyx",
   "collapsed_sections": [],
   "name": "Capstone Project (Machine Learning Models)- Automatic Ticket Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
