{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditya Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package brown to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "# for all NLP related operations on text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "# To mock web-browser and scrap tweets\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# To consume Twitter's API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler \n",
    "\n",
    "# To identify the sentiment of text\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "# ignoring all the warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# downloading stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('brown')\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# for showing all the plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tickets are  (48549, 2)\n"
     ]
    }
   ],
   "source": [
    "Tickets=pd.read_csv('all_tickets.csv')\n",
    "print ('Number of Tickets are ', Tickets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    object\n",
       "body     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tickets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>hi since recruiter lead permission approve req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>connection with icon</td>\n",
       "      <td>icon dear please setup icon per icon engineers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>work experience user</td>\n",
       "      <td>work experience user hi work experience studen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>requesting for meeting</td>\n",
       "      <td>requesting meeting hi please help follow equip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reset passwords for external accounts</td>\n",
       "      <td>re expire days hi ask help update passwords co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  \\\n",
       "0                                    NaN   \n",
       "1                   connection with icon   \n",
       "2                   work experience user   \n",
       "3                 requesting for meeting   \n",
       "4  reset passwords for external accounts   \n",
       "\n",
       "                                                body  \n",
       "0  hi since recruiter lead permission approve req...  \n",
       "1  icon dear please setup icon per icon engineers...  \n",
       "2  work experience user hi work experience studen...  \n",
       "3  requesting meeting hi please help follow equip...  \n",
       "4  re expire days hi ask help update passwords co...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tickets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Analysis of Tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94mtitle    712\n",
      "body       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Finding Null Values\n",
    "print(color.BOLD + color.BLUE + str(Tickets.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94mMax description length for Tickets is 7011\n"
     ]
    }
   ],
   "source": [
    "#Length of 'Tickets'\n",
    "Tickets_lengths = [len(x) for x in Tickets['body']]\n",
    "\n",
    "print (color.BOLD + color.BLUE +  'Max description length for Tickets is {}'.format(max(Tickets_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>10%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>95%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48549.0</td>\n",
       "      <td>266.640178</td>\n",
       "      <td>384.651236</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>896.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>7011.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count        mean         std  min   10%   25%    50%    75%    95%  \\\n",
       "0  48549.0  266.640178  384.651236  2.0  56.0  88.0  150.0  275.0  896.0   \n",
       "\n",
       "      99%     max  \n",
       "0  1900.0  7011.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Destribution of lengths\n",
    "pd.DataFrame(Tickets_lengths).describe(percentiles=[0.1, .25, .5, .75, .95, .99]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94mDistribution of Lenghts [Top 99%]\n"
     ]
    }
   ],
   "source": [
    "sns.set(style='whitegrid')\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "print(color.BOLD + color.BLUE + \"Distribution of Lenghts [Top 99%]\")\n",
    "sns.distplot(list(filter(lambda x:x<np.percentile(Tickets_lengths,99),Tickets_lengths)), kde=False)\n",
    "plt.show()\n",
    "figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "print(color.BOLD + color.BLUE + \"Distribution of Lenghts [All]\")\n",
    "sns.distplot(Tickets_lengths, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal frequency of Tickets¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set(title='Temporal Tickets frequency', xlabel='Time', ylabel='Tickets frequency per hour')\n",
    "plt.hist(Tickets.index, bins = 24*9, color = 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing URLs\n",
    "Tickets['body'] = Tickets['body'].apply(lambda x: re.sub(r\"https\\S+\", \"\", str(x)))\n",
    "\n",
    "# Converting all tweets to lowercase¶\n",
    "Tickets['body'] = Tickets['body'].apply(lambda x: x.lower())\n",
    "\n",
    "#Carriage Returns & Tabs\n",
    "Tickets['body'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\" \",\" \"], regex=True, inplace=True)\n",
    "\n",
    "#Special Characters\n",
    "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
    "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
    "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
    "              \"`\",\"{\",\"|\",\"}\",\"~\",\"–\",\"-\"]\n",
    "for char in spec_chars:\n",
    "    Tickets['body'] = Tickets['body'].apply(lambda x:x.replace(char, ' '))\n",
    "\n",
    "#Extra spaces\n",
    "Tickets['body'].replace('\\s+', ' ', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non ASCII words\n",
    "Tickets['body']=Tickets['body'].apply(lambda x:x.encode(\"ascii\",\"ignore\").decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing StopWords\n",
    "nltk.download('stopwords')\n",
    "stop=set(stopwords)\n",
    "stop.update(['please','know',\n",
    "'high','monday','assist','st','file','requested','created','added','per','well','next','approve','care','item','back','submitted','meeting','annual'\n",
    "'monday','hub','engineer','name','analyst','code','number','ticket','approved','available','head','delete','view','task','status','leaver''monday','problem','va','importance','developer','submit','users','ad','one','ask','take','note','old','guys','id','po','advise','assign','lead','id','po','today','need','test','ca','make','mailbox','required','received','phone','needed','user','receive','assigned','days','february','create','error','november', 'request', 'kindly', 'covid19', 'requesting','thank','thanks','regards','best','en','he','please','hi','please','dear','help','kindly','kind','hello','super','much','advised','pm','sent','tuesday','wednesday','thursday','friday','saturday','july','October','November','could','ext','issue','let','log','attached','change','october','date','also','information','senior','add','form','details','december','provide','leave','march','si','issues','manager','working','cannot','find','la'])\n",
    "\n",
    "Tickets['body-tokenized']=Tickets['body'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "columnsToDropDuplicates = ['body']\n",
    "Tickets = Tickets.drop_duplicates(columnsToDropDuplicates)\n",
    "print(Tickets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import contractions\n",
    "\n",
    "# data['Long_Desc'] = data['Long_Desc'].apply(lambda v: contractions.fix(v))\n",
    "# sentences = data[\"Long_Desc\"].progress_apply(lambda x: x.split()).values\n",
    "# cleaned_vocab = build_vocab(sentences)\n",
    "# oov = check_coverage(cleaned_vocab,bert_vocab)\n",
    "\n",
    "# data['Short_Desc'] = data['Short_Desc'].apply(lambda v: contractions.fix(v))\n",
    "# sentences = data[\"Short_Desc\"].progress_apply(lambda x: x.split()).values\n",
    "# cleaned_vocab = build_vocab(sentences)\n",
    "# oov = check_coverage(cleaned_vocab,bert_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for cleaning\n",
    "def cleanDataset(dataset, columnsToClean, regexList):\n",
    "    for column in columnsToClean:\n",
    "        for regex in regexList:\n",
    "            dataset[column] = removeString(dataset[column], regex)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def getRegexList():\n",
    "    regexList = []\n",
    "    regexList += ['From:(.*)\\r\\n']  # from line\n",
    "    # regexList += ['RITM[0-9]*'] # request id\n",
    "    # regexList += ['INC[0-9]*'] # incident id\n",
    "    # regexList += ['TKT[0-9]*'] # ticket id\n",
    "    regexList += ['Sent:(.*)\\r\\n']  # sent to line\n",
    "    regexList += ['Received:(.*)\\r\\n']  # received data line\n",
    "    regexList += ['To:(.*)\\r\\n']  # to line\n",
    "    regexList += ['CC:(.*)\\r\\n']  # cc line\n",
    "    regexList += ['The information(.*)infection']  # footer\n",
    "    regexList += ['Endava Limited is a company(.*)or omissions']  # footer\n",
    "    regexList += ['The information in this email is confidential and may be legally(.*)interference if you are not the intended recipient']  # footer\n",
    "    regexList += ['\\[cid:(.*)]']  # images cid\n",
    "    regexList += ['https?:[^\\]\\n\\r]+']  # https & http\n",
    "    regexList += ['Subject:']\n",
    "    # regexList += ['[\\w\\d\\-\\_\\.]+@[\\w\\d\\-\\_\\.]+']  # emails\n",
    "    # regexList += ['[0-9][\\-0–90-9 ]+']  # phones\n",
    "    # regexList += ['[0-9]']  # numbers\n",
    "    # regexList += ['[^a-zA-z 0-9]+']  # anything that is not a letter\n",
    "    # regexList += ['[\\r\\n]']  # \\r\\n\n",
    "    # regexList += [' [a-zA-Z] ']  # single letters\n",
    "    # regexList += [' [a-zA-Z][a-zA-Z] ']  # two-letter words\n",
    "    # regexList += [\"  \"]  # double spaces\n",
    "\n",
    "    regexList += ['^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$']\n",
    "    regexList += ['[\\w\\d\\-\\_\\.]+ @ [\\w\\d\\-\\_\\.]+']\n",
    "    regexList += ['Subject:']\n",
    "    regexList += ['[^a-zA-Z]']\n",
    "\n",
    "    return regexList\n",
    "\n",
    "def removeString(data, regex):\n",
    "    return data.str.lower().str.replace(regex.lower(), ' ')\n",
    "\n",
    "\n",
    "columnsToClean = ['body', 'title']\n",
    "\n",
    "# Create list of regex to remove sensitive data\n",
    "# Clean dataset and remove sensitive data\n",
    "cleanDataset(Tickets, columnsToClean, getRegexList())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def generate_ngrams(text, n_gram=1):\n",
    "    token = [token for token in text.lower().split(' ') if token != '' if token not in stop]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "def show_top_unigrams(column_name):\n",
    "    # Unigrams\n",
    "    incidents_unigrams = collections.defaultdict(int)\n",
    "\n",
    "    for tweet in Tickets[column_name]:\n",
    "        for word in generate_ngrams(tweet):\n",
    "            incidents_unigrams[word] += 1\n",
    "\n",
    "    df_incidents_unigrams = pd.DataFrame(sorted(incidents_unigrams.items(), key=lambda x: x[1])[::-1])       \n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(18, 50), dpi=100)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    N = 50\n",
    "\n",
    "    sns.barplot(y=df_incidents_unigrams[0].values[:N], x=df_incidents_unigrams[1].values[:N], ax=axes[0], color='red')\n",
    "\n",
    "    axes[0].spines['right'].set_visible(False)\n",
    "    axes[0].set_xlabel('')\n",
    "    axes[0].set_ylabel('')\n",
    "    axes[0].tick_params(axis='x', labelsize=13)\n",
    "    axes[0].tick_params(axis='y', labelsize=13)\n",
    "\n",
    "    axes[0].set_title(f'Top {N} most common unigrams in Incidents Description', fontsize=20)\n",
    "    \n",
    "    df_incidents_unigrams.to_csv('unigrams.csv', index=False, index_label=False)\n",
    "\n",
    "    plt.show() \n",
    "    \n",
    "show_top_unigrams('body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_unigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to generate WordCloud \n",
    "  \n",
    "# importing all necessery modules \n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads 'Youtube04-Eminem.csv' file  \n",
    "df = pd.read_csv(r\"unigrams.csv\", encoding =\"latin-1\") \n",
    "\n",
    "comment_words = '' \n",
    "stopwords = set(STOPWORDS) \n",
    "\n",
    "df.columns=['a','b']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the csv file \n",
    "for val in df.a: \n",
    "      \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "  \n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(comment_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
