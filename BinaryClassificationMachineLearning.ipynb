{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditya Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package brown to C:\\Users\\Aditya\n",
      "[nltk_data]     Jain\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "# for all NLP related operations on text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "# To mock web-browser and scrap tweets\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# To consume Twitter's API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler \n",
    "\n",
    "# To identify the sentiment of text\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "# ignoring all the warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# downloading stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('brown')\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# for showing all the plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_predict = \"ticket_type\"\n",
    "# Supported datasets:\n",
    "# ticket_type\n",
    "# business_service\n",
    "# category\n",
    "# impact\n",
    "# urgency\n",
    "# sub_category1\n",
    "# sub_category2\n",
    "\n",
    "classifier = \"NB\"  # Supported algorithms # \"SVM\" # \"NB\"\n",
    "use_grid_search = True  # grid search is used to find hyperparameters. Searching for hyperparameters is time consuming\n",
    "remove_stop_words = True  # removes stop words from processed text\n",
    "stop_words_lang = 'english'  # used with 'remove_stop_words' and defines language of stop words collection\n",
    "use_stemming = False  # word stemming using nltk\n",
    "fit_prior = True  # if use_stemming == True then it should be set to False ?? double check\n",
    "min_data_per_class = 1  # used to determine number of samples required for each class.Classes with less than that will be excluded from the dataset. default value is 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset from dprep in Workbench    \n",
    "# dfTickets = package.run('AllTickets.dprep', dataflow_idx=0) \n",
    "\n",
    "# loading dataset from csv\n",
    "dfTickets = pd.read_csv(\n",
    "    'all_ticketsnew.csv',\n",
    "    dtype=str\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select 'TEXT' column and remove poorly represented classes¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset before removing classes with less then 1 rows: (48549, 10)\n",
      "Number of classes before removing classes with less then 1 rows: 2\n",
      "Shape of dataset after removing classes with less then 1 rows: (48549, 10)\n",
      "Number of classes after removing classes with less then 1 rows: 2\n"
     ]
    }
   ],
   "source": [
    "text_columns = \"body\"  # \"title\" - text columns used for TF-IDF\n",
    "\n",
    "# Removing rows related to classes represented by low amount of data\n",
    "print(\"Shape of dataset before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(dfTickets.shape))\n",
    "print(\"Number of classes before removing classes with less then \" + str(min_data_per_class) + \" rows: \"+str(len(np.unique(dfTickets[column_to_predict]))))\n",
    "bytag = dfTickets.groupby(column_to_predict).aggregate(np.count_nonzero)\n",
    "tags = bytag[bytag.body > min_data_per_class].index\n",
    "Tickets = dfTickets[dfTickets[column_to_predict].isin(tags)]\n",
    "print(\n",
    "    \"Shape of dataset after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(Tickets.shape)\n",
    ")\n",
    "print(\n",
    "    \"Number of classes after removing classes with less then \"\n",
    "    + str(min_data_per_class) + \" rows: \"\n",
    "    + str(len(np.unique(Tickets[column_to_predict])))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data and labels and split them to train and test sets¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelData = Tickets[column_to_predict]\n",
    "data = Tickets[text_columns]\n",
    "\n",
    "# Split dataset into training and testing data\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labelData, test_size=0.2\n",
    ")  # split data to train/test sets with 80:20 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from text¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38839, 11239)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorizer\n",
    "if remove_stop_words:\n",
    "    count_vect = CountVectorizer(stop_words=stop_words_lang)\n",
    "elif use_stemming:\n",
    "    count_vect = StemmedCountVectorizer(stop_words=stop_words_lang)\n",
    "else:\n",
    "    count_vect = CountVectorizer()\n",
    "\n",
    "vectorized_data = count_vect.fit_transform(train_data)\n",
    "vectorized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38839, 11239)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "features = tfidf.fit_transform(vectorized_data)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pipeline to preprocess data and train classifier¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NB classifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Fitting the training data into a data processing pipeline and eventually into the model itself\n",
    "if classifier == \"NB\":\n",
    "    print(\"Training NB classifier\")\n",
    "    # Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "    # The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "    # We will be using the 'text_clf' going forward.\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB(fit_prior=fit_prior))\n",
    "    ])\n",
    "    text_clf = text_clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GridSearchCV to search for best set of parameters¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if use_grid_search:\n",
    "    # Grid Search\n",
    "    # Here, we are creating a list of parameters for which we would like to do performance tuning.\n",
    "    # All the parameters name start with the classifier name (remember the arbitrary name we gave).\n",
    "    # E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "    # NB parameters\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'clf__alpha': (1e-2, 1e-3)\n",
    "    }\n",
    "\n",
    "    # Next, we create an instance of the grid search by passing the classifier, parameters\n",
    "    # and n_jobs=-1 which tells to use multiple cores from user machine.\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    gs_clf = gs_clf.fit(train_data, train_labels)\n",
    "\n",
    "    # To see the best mean score and the params, run the following code\n",
    "    gs_clf.best_score_\n",
    "    gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "Confusion matrix without GridSearch:\n",
      "[[2623  128]\n",
      " [  90 6869]]\n",
      "Mean without GridSearch: 0.9775489186405767\n",
      "Confusion matrix with GridSearch:\n",
      "[[2686   65]\n",
      " [  91 6868]]\n",
      "Mean with GridSearch: 0.9839340885684861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Evaluating model\")\n",
    "# Score and evaluate model on test data using model without hyperparameter tuning\n",
    "predicted = text_clf.predict(test_data)\n",
    "prediction_acc = np.mean(predicted == test_labels)\n",
    "print(\"Confusion matrix without GridSearch:\")\n",
    "print(confusion_matrix(test_labels, predicted))\n",
    "print(\"Mean without GridSearch: \" + str(prediction_acc))\n",
    "\n",
    "# Score and evaluate model on test data using model WITH hyperparameter tuning\n",
    "if use_grid_search:\n",
    "    predicted = gs_clf.predict(test_data)\n",
    "    prediction_acc = np.mean(predicted == test_labels)\n",
    "    print(\"Confusion matrix with GridSearch:\")\n",
    "    print(confusion_matrix(test_labels, predicted))\n",
    "    print(\"Mean with GridSearch: \" + str(prediction_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting confusion matrix¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting confusion matrix with 'seaborn' module\n",
    "# Use below line only with Jupyter Notebook\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "mat = confusion_matrix(test_labels, predicted)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set()\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=np.unique(test_labels),\n",
    "            yticklabels=np.unique(test_labels))\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "# Save confusion matrix to outputs in Workbench\n",
    "# plt.savefig(os.path.join('.', 'outputs', 'confusion_matrix.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing classification report¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      2751\n",
      "           1       0.99      0.99      0.99      6959\n",
      "\n",
      "    accuracy                           0.98      9710\n",
      "   macro avg       0.98      0.98      0.98      9710\n",
      "weighted avg       0.98      0.98      0.98      9710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, predicted,\n",
    "                            target_names=np.unique(test_labels.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
